import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
from sklearn.model_selection import cross_val_score, StratifiedKFold
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from scripts.data_preprocessing import preprocess_pipeline


def get_models():
    models = {
        'Random Forest': RandomForestClassifier(
            n_estimators=100, max_depth=10, min_samples_split=5,
            min_samples_leaf=2, random_state=42, class_weight='balanced'
        ),
        'Logistic Regression': LogisticRegression(
            penalty='l2', solver='liblinear', max_iter=1000, 
            class_weight='balanced', random_state=42
        ),
        'Gradient Boosting': GradientBoostingClassifier(
            n_estimators=100, learning_rate=0.1, max_depth=3, 
            random_state=42
        ),
        'SVM (RBF)': SVC(
            kernel='rbf', C=1.0, gamma='scale', 
            class_weight='balanced', random_state=42, probability=True
        )
    }
    return models


def evaluate_models_cv(models, X, y, cv_folds=5):
    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    results = {}
    for name, model in models.items():
        print(f"Evaluating {name}...")
        
        # Cross-validation scores
        cv_accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
        cv_f1 = cross_val_score(model, X, y, cv=cv, scoring='f1')
        cv_roc_auc = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
        
        results[name] = {
            'accuracy_mean': cv_accuracy.mean(),
            'accuracy_std': cv_accuracy.std(),
            'f1_mean': cv_f1.mean(),
            'f1_std': cv_f1.std(),
            'roc_auc_mean': cv_roc_auc.mean(),
            'roc_auc_std': cv_roc_auc.std(),
            'cv_accuracy_scores': cv_accuracy,
            'cv_f1_scores': cv_f1,
            'cv_roc_auc_scores': cv_roc_auc
        }
    
    return results


def evaluate_models_holdout(models, X_train, X_test, y_train, y_test):
    holdout_results = {}
    
    for name, model in models.items():
        # Train model
        model.fit(X_train, y_train)
        
        # Make predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        # Calculate metrics
        holdout_results[name] = {
            'accuracy': accuracy_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred),
            'roc_auc': roc_auc_score(y_test, y_pred_proba),
            'model': model  # Keep trained model for later use
        }
    
    return holdout_results


def create_comparison_visualizations(cv_results, holdout_results, output_dir="outputs"):
    # Set up the plot
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Model Comparison Results', fontsize=16)
    
    # Extract data for plotting
    model_names = list(cv_results.keys())
    
    # 1. Cross-validation accuracy comparison
    cv_accuracies = [cv_results[model]['cv_accuracy_scores'] for model in model_names]
    axes[0, 0].boxplot(cv_accuracies, labels=model_names)
    axes[0, 0].set_title('Cross-Validation Accuracy Distribution')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].tick_params(axis='x', rotation=45)
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Cross-validation F1 comparison
    cv_f1_scores = [cv_results[model]['cv_f1_scores'] for model in model_names]
    axes[0, 1].boxplot(cv_f1_scores, labels=model_names)
    axes[0, 1].set_title('Cross-Validation F1 Score Distribution')
    axes[0, 1].set_ylabel('F1 Score')
    axes[0, 1].tick_params(axis='x', rotation=45)
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Holdout performance comparison
    metrics = ['accuracy', 'f1_score', 'roc_auc']
    holdout_data = []
    for metric in metrics:
        holdout_data.append([holdout_results[model][metric] for model in model_names])
    
    x = np.arange(len(model_names))
    width = 0.25
    
    for i, metric in enumerate(metrics):
        axes[1, 0].bar(x + i*width, holdout_data[i], width, label=metric.replace('_', ' ').title())
    
    axes[1, 0].set_xlabel('Models')
    axes[1, 0].set_ylabel('Score')
    axes[1, 0].set_title('Holdout Test Performance')
    axes[1, 0].set_xticks(x + width)
    axes[1, 0].set_xticklabels(model_names, rotation=45)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Mean CV performance with error bars
    cv_means = [cv_results[model]['accuracy_mean'] for model in model_names]
    cv_stds = [cv_results[model]['accuracy_std'] for model in model_names]
    
    axes[1, 1].bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.7)
    axes[1, 1].set_title('Cross-Validation Accuracy (Mean ± Std)')
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].tick_params(axis='x', rotation=45)
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save the plot
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(f"{output_dir}/model_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()


def run_model_comparison(file_path):
    """Run complete model comparison pipeline"""
    print("Starting Model Comparison Pipeline...")
    print("=" * 60)
    
    # Load and preprocess data
    print("1. Loading and preprocessing data...")
    X_train, X_test, y_train, y_test = preprocess_pipeline(file_path)
    
    # Select features
    selected_features = [
        'concave points_mean','area_worst','fractal_dimension_worst','smoothness_worst','symmetry_worst',
        'fractal_dimension_mean','smoothness_mean','compactness_se','fractal_dimension_se','concave points_se',
        'symmetry_se','perimeter_se','concavity_se','symmetry_mean','smoothness_se','texture_se','texture_mean'
    ]
    
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]
    
    # Combine training data for cross-validation
    X_full = pd.concat([X_train_selected, X_test_selected])
    y_full = pd.concat([y_train, y_test])
    
    print(f"   - Total samples: {len(X_full)}")
    print(f"   - Features used: {len(selected_features)}")
    print(f"   - Class distribution: {y_full.value_counts().to_dict()}")
    
    # Get models
    models = get_models()
    print(f"\n2. Models to compare: {list(models.keys())}")
    
    # Cross-validation evaluation
    print("\n3. Running cross-validation evaluation...")
    cv_results = evaluate_models_cv(models, X_full, y_full, cv_folds=5)
    
    # Holdout evaluation  
    print("\n4. Running holdout evaluation...")
    holdout_results = evaluate_models_holdout(models, X_train_selected, X_test_selected, y_train, y_test)
    
    # Print results
    print("\n5. Cross-Validation Results:")
    print("=" * 40)
    cv_summary = []
    for model_name, results in cv_results.items():
        print(f"\n{model_name}:")
        print(f"  Accuracy: {results['accuracy_mean']:.4f} ± {results['accuracy_std']:.4f}")
        print(f"  F1 Score: {results['f1_mean']:.4f} ± {results['f1_std']:.4f}")
        print(f"  ROC AUC:  {results['roc_auc_mean']:.4f} ± {results['roc_auc_std']:.4f}")
        
        cv_summary.append({
            'Model': model_name,
            'CV_Accuracy_Mean': results['accuracy_mean'],
            'CV_Accuracy_Std': results['accuracy_std'],
            'CV_F1_Mean': results['f1_mean'],
            'CV_F1_Std': results['f1_std'],
            'CV_ROC_AUC_Mean': results['roc_auc_mean'],
            'CV_ROC_AUC_Std': results['roc_auc_std']
        })
    
    print("\n6. Holdout Test Results:")
    print("=" * 40)
    holdout_summary = []
    for model_name, results in holdout_results.items():
        print(f"\n{model_name}:")
        print(f"  Accuracy: {results['accuracy']:.4f}")
        print(f"  F1 Score: {results['f1_score']:.4f}")
        print(f"  ROC AUC:  {results['roc_auc']:.4f}")
        
        holdout_summary.append({
            'Model': model_name,
            'Holdout_Accuracy': results['accuracy'],
            'Holdout_F1': results['f1_score'],
            'Holdout_ROC_AUC': results['roc_auc']
        })
    
    # Save results
    print("\n7. Saving results...")
    os.makedirs("outputs", exist_ok=True)
    
    cv_df = pd.DataFrame(cv_summary)
    holdout_df = pd.DataFrame(holdout_summary)
    
    cv_df.to_csv("outputs/cv_comparison_results.csv", index=False)
    holdout_df.to_csv("outputs/holdout_comparison_results.csv", index=False)
    
    # Create visualizations
    print("\n8. Creating comparison visualizations...")
    create_comparison_visualizations(cv_results, holdout_results)
    
    # Determine best model
    print("\n9. Model Ranking:")
    print("=" * 40)
    
    # Rank by CV ROC AUC (most robust metric)
    cv_df_sorted = cv_df.sort_values('CV_ROC_AUC_Mean', ascending=False)
    print("\nBased on Cross-Validation ROC AUC:")
    for i, (_, row) in enumerate(cv_df_sorted.iterrows(), 1):
        print(f"{i}. {row['Model']}: {row['CV_ROC_AUC_Mean']:.4f} ± {row['CV_ROC_AUC_Std']:.4f}")
    
    print(f"\nRecommended model: {cv_df_sorted.iloc[0]['Model']}")
    print(f"Your Random Forest ranks: #{cv_df_sorted[cv_df_sorted['Model'] == 'Random Forest'].index[0] + 1}")
    
    print("\n" + "=" * 60)
    print("Model Comparison Complete!")
    
    return cv_results, holdout_results, cv_df_sorted


if __name__ == "__main__":
    cv_results, holdout_results, rankings = run_model_comparison("data/breast-cancer.csv")